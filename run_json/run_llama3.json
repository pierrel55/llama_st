// llm run parameters. note: this file must be saved in utf-8 format
{
// ------------------------------------
// model identifier

"model_ident": "llama3",        // define model type for model specificities, refer to model.c for list

// ------------------------------------
// model load

"model_num_safetensors": 4,                   // count of .safetensors files
"model_path": "D:/llama3_st/8b-instruct",     // path to .safetensors, config.json

// name of tokenizer
"tokenizer_name": "",        // if empty: model_path/tokenizer.json used else define full path+name

// ------------------------------------
// transformer parameters

// rope value
"rope_set": 0,               // 0:use config.json or .safetensors data, >0:user value ex:10000.0 (llama2 value)

// ------------------------------------
// sampler parameters
"temperature": 1.0,          // 0.0 to 2.0: 0.0:greedy decoding 2.0:maximum creativity (1.0 = disable)
"topp": 0.65,                // 0.01 to 0.99: max probability sum of top tokens
"topk": 40,                  // (integer) limit size of top tokens list 5..200 (0 = disable)
"topp_minp": 0.05,           // 0.0 to 1.0: (experimental) !=0: min token probability required to continue generate if EOS contained in topp list
"topp_eos": false,           // true: limit topp list size to token with probability >= EOS
"repeat_penalty": 0.0,       // 0.0..2.0 repeat penalty (0.0 = disable)
"repeat_penalty_n": 0,       // (integer) count of last generated tokens used to apply repeat penalty (0 = disable, min 10)
"eos_amp": 0.5,              // 0.0 to 2.0 amplify eos probability when more than eos_amp_n tokens generated. (0 = disable)
"eos_amp_n": 250,            // (integer) count of tokens generated before starting eos_amp influence (0 = disable, min 10)
"rand_seed": 1234,           // (integer) random seed

// (optional) if ch_restrict defined, allow to sample only tokens that contain ascii chars and utf8 chars contained in sample_restrict string.
// "ch_restrict": "ÇüéâäàåçêëèïîìÄÅÉæÆôöòûùÿÖÜ¢£¥₧ƒáíóúñÑªº¿⌐¬½¼¡«»αßΓπΣσµτΦΘΩδ∞φε∩≡±≥≤⌠⌡÷≈°∙·√ⁿ²",

"test_nan_logits": false,    // test for NAN at sampling in logits result

// ------------------------------------

// model load data conversion
"cvt_sf16": false,           // convert model to sf16 (require f16 model)
"cvt_f12": false,            // convert model to float12 (should be possible with all models, require all weights <= 4.0)
"cvt_f8": false,             // convert model to float8 (not possible with some models, require all weights <= 2.0)

// hardware parameters
"num_procs": 12,             // -1: max auto detected (may be adjusted), >0: user value. note: max procs do not always produce best performances
"numa_nodes": -1,            // -1: use all detected. 0: skip numa specific code, >0: max nodes to use
"simd_mode": -1,             // -1: max detect, 0:fpu 1:sse 2:avx, 3:avx2

// run parameters
"run_mode": 0,                         // 0: generate, 1:chat
"gen_run_steps": -1,                   // generate mode run steps: <=0: model max context size, >0:user value
"token_eos_str": "<|eot_id|>",         // end of string token (assistant reply end)
"token_eot_str": "<|end_of_text|>",    // end of text token (dialog/generate end)

// tokens display options in chat or generate mode
"tok_disp_raw": false,       // true: display control/byte for tokens (LF,<s>, etc)
"tok_disp_split":false,      // true: display token list separated with ','
"tok_disp_prob": false,      // true: display sampling info (add [score + n topp] + ',')

// ------------------------------------
// generate mode config

"gen_mode_prompt": "<|begin_of_text|>The explanation for the existence of seasons is",

// ------------------------------------
// chat mode config

// dialog colors (r.g.b format)
"chat_use_colors": true,               // use colors for chat
"chat_col_msg":       "250.250.250",   // messages text color
"chat_col_user":      "180.255.180",   // user text color (keyboard input)
"chat_col_assistant": "180.180.255",   // assistant answer text color

// forward: define what is displayed when forward user prompt
"fwd_disp_mode": 0,                    // 0: display nothing, 1:tokens list (test/check mode)

// ------------------------------------
// promp mode: define the method to generate the prompt format
// 0: use model_ident value to select templates defined in chat.c
// 1: user defined templates cm1_xxx..
// 2: use generate mode (llama1), should work with any models (chat/non-chat).
"chat_prompt_mode": 0,                 // 0: use model_ident value to define templates. 1, user defined template, 2 use generate mode (llama1)

// prompt names displayed for assistant and user in mode 0 and 1 (defined by sys_prompt in mode 2)
"chat_assistant_name": "LLama3:",
"chat_user_name": "User:",

// ------------------------------------
// chat_prompt_mode=0 parameters

"cm0_sys_prompt": "You are a helpful AI assistant for travel tips and recommendations",
"cm0_user_prompt": "What is France's capital?",

// ------------------------------------
// chat_prompt_mode=1 parameters (user defined template)

// https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/
// https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1
"cm1_sys_template": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n%s<|eot_id|>",
"cm1_user_first_template": "",
"cm1_user_template": "<|start_header_id|>user<|end_header_id|>\n\n%s<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
"cm1_end_template": "<|eot_id|>\n",

"cm1_sys_prompt": "You are a helpful assistant.",
"cm1_user_prompt": "Explain shortly what is a pointer in C language ?",

// ------------------------------------
// init prompt mode 2 (generate mode)
// for mode 2 correct work:
//  - ensure defined names coherences in user_template/sys_prompt/user_name_sw
//  - ensure no space at end of user_name_sw
//  - terminate sys prompt to user name

// templates required for mode 1 or 2
"cm2_sys_template": "<|begin_of_text|>%s",     // %s replaced by cm2_sys_prompt, <s> = emit bos
"cm2_user_template": " %s\nBob:",              // %s = cm2_user_prompt at init and next with keyboard input string
// user name switch, required for mode 2 only, string detect switch to user in sys prompt (size 5..10 char, no space at end)
"cm2_user_name_sw": "\nUser:",

// here is llama.cpp project llama1 chat example
"cm2_sys_prompt":
     "Transcript of a dialog, where the User interacts with an Assistant named Bob. "
    +"Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\n\n"
    +"User: Hello, Bob.\n"
    +"Bob: Hello. How may I help you today?\n"
    +"User:",
"cm2_user_prompt": "Do you know what is the first prime number greater than 15 ?"
}
