read file run_json/run_codellama.json
CPU flags: f16c:1 fma3:1, sse4.2:1 avx:1 avx2:1
conv/matmul AVX2 checks done.
load tokenizer: E:/codellama/codellama-7b-instruct-hf/tokenizer.json
load transformer..
read model config in: E:/codellama/codellama-7b-instruct-hf/config.json
torch float type: bf16
numa node(s): 1, mp node: 0, num logical/physical procs.: 14/14 (HT off)
processor(s) core(s) used: 12 in 1 node(s).
load: E:/codellama/codellama-7b-instruct-hf/model-00001-of-00002.safetensors
load: E:/codellama/codellama-7b-instruct-hf/model-00002-of-00002.safetensors
sampler config:
  temperature      : 0.60
  topp             : 0.90
  topk             : 30
  topp_minp        : 0.05
  topp_eos         : true
  repeat_penalty   : 0.00
  repeat_penalty_n : 50
  eos_amp          : 0.50
  eos_amp_n        : 150
  rand seed        : 1234
Generate: max 16384 tokens..
- Press 'esc' key to break generation.
bool is_prime(int x)
{
    if (x < 2)
        return false;

    if (x % 2 == 0)
        return x == 2;

    int root = (int)std::sqrt(x);
    for (int i = 3; i <= root; i += 2)
        if (x % i == 0)
            return false;

    return true;
}

int main()
{
    int n;
    cin >> n;

    vector<int> nums(n);
    for (int i = 0; i < n; ++i)
    {
        cin >> nums[i];
    }

    int count = 0;
    for (int i = 0; i < n; ++i)
    {
        if (is_prime(nums[i]))
            count++;
    }

    cout << count;

    return 0;
}
total time: 45.26s for 231 tokens, tok/s: 5.10
Press any key to continue . . .